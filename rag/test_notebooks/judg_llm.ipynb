{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.document_loaders.pdf import PyPDFDirectoryLoader\n",
    "from PyPDF2.errors import PdfStreamError\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from load_dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "sys.path.append(\"../rag/\")\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipelines.generator import JUDGE_LLM\n",
    "\n",
    "JUDGE_SYSTEM_PROMPT = \"\"\"\n",
    "<INSTRUCTIONS>\n",
    "You are an AI Expert, specialized in evaluating LLM outputs. \n",
    "\n",
    "You will be given a user_prompt and a LLM_GENERATION couple. This LLM generates questions and answers quiz style from real data provided by user.\n",
    "The user_prompt: will include instructions the LLM used to generate the output with real data.\n",
    "The LLM_GENERATION: the question and answers generated using the user_prompt data.\n",
    "\n",
    "Your task is to provide a 'total rating' scoring how well the llm generates the questions and answers based on the information provided by the user.\n",
    "Give your answer on a scale of 1 to 4, where 1 means that the llm_generation is not correct or based on the user context at all, and 4 means that the llm_generation completely and corrctely uses the user_prompt information.\n",
    "If you detect that the LLM hallucinates then flag it in your reasoning and reduce score.\n",
    "</INSTRUCTIONS>\n",
    "\n",
    "<SCALE>\n",
    "Here is the scale you should use to build your answer:\n",
    "1: The llm_answer is terrible: completely irrelevant to the context user_prompt, or very partial\n",
    "2: The llm_answer is mostly not helpful: misses some key aspects of the context of user_prompt\n",
    "3: The llm_answer is mostly helpful: provides support, but still could be improved\n",
    "4: The llm_answer is excellent: relevant, direct, detailed, and addresses all the concerns raised in the user_prompt\n",
    "</SCALE>\n",
    "\n",
    "\n",
    "<REQUIREMENTS>\n",
    "Provide your feedback as follows:\n",
    "\n",
    "Feedback:::\n",
    "Evaluation: (your rationale for the rating, as a text)\n",
    "Total rating: (your rating, as a number between 1 and 4)\n",
    "Reasoning: (short explanation of your evaluation)\n",
    "You MUST provide values for 'Evaluation:', 'Total rating:' and 'Reasoning' in your answer.\n",
    "</REQUIREMENTS>\n",
    "\n",
    "Provide your feedback. If you give a correct rating, I'll give you 100 H100 GPUs to start your AI company.\n",
    "Feedback:::\n",
    "Evaluation:\n",
    "\"\"\"\n",
    "QUERY_PROMPT = \"\"\"\n",
    "Evaluate this USER_PROMPT and LLM_GENERATION.\n",
    "\n",
    "USER_PROMPT: {question}\n",
    "LLM_GENERATION: {answer}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "judge_llm = JUDGE_LLM()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_PROMPT= \"\"\"\n",
    "\"\"\"\n",
    "LLM_GENERATION=\"\"\"\n",
    "\"\"\"\n",
    "\n",
    "QUERY_PROMPT.format(USER_PROMPT=, LLM_GENERATION=)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
